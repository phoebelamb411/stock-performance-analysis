{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96c33a29",
   "metadata": {},
   "source": [
    "# ðŸ“Š Stock Performance & Investment Analysis (2025)\n",
    "### Portfolio Optimization Â· Risk-Adjusted Returns Â· Investment Strategy Comparison Â· Financial Modeling\n",
    "\n",
    "---\n",
    "\n",
    "I wanted to build a project that combined financial modeling with the SQL and Python skills I've been developing in class. The idea is to analyze stock performance across a few different sectors, then actually try to build and optimize portfolios â€” not just look at returns, but really dig into the risk-adjusted side of things.\n",
    "\n",
    "**Technologies:** Python (pandas, NumPy, SciPy, Matplotlib, Seaborn) Â· SQL (SQLite)\n",
    "\n",
    "**Key Techniques:** Markowitz Portfolio Optimization Â· Sharpe & Sortino Ratios Â· Value at Risk (VaR) Â· Maximum Drawdown Â· Momentum & Risk-Parity Strategies Â· Cholesky Decomposition for Correlated Return Simulation\n",
    "\n",
    "---\n",
    "\n",
    "### Table of Contents\n",
    "1. [Setup & Configuration](#1-setup--configuration)\n",
    "2. [Data Acquisition & Validation](#2-data-acquisition--validation)\n",
    "3. [SQL Database Layer](#3-sql-database-layer--storage--querying)\n",
    "4. [Exploratory Performance Analysis](#4-exploratory-performance-analysis)\n",
    "5. [Correlation & Sector Analysis](#5-correlation--sector-analysis)\n",
    "6. [Portfolio Optimization (Markowitz)](#6-portfolio-optimization-markowitz-mean-variance)\n",
    "7. [Risk Metrics & Drawdown Analysis](#7-risk-metrics--drawdown-analysis)\n",
    "8. [Investment Strategy Comparison](#8-investment-strategy-comparison)\n",
    "9. [Executive Summary](#9-executive-summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee150f65",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Configuration\n",
    "\n",
    "I'm starting by pulling in my libraries and laying out the stock universe I want to work with. I picked 8 stocks spread across 4 sectors â€” tech, financials, healthcare, energy, and consumer staples â€” plus SPY as my market benchmark. That way I have enough diversity to actually see differences when I start comparing portfolios later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93f9669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SETUP & CONFIGURATION\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import norm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setting the seed so my results are reproducible every time I re-run\n",
    "np.random.seed(42)\n",
    "\n",
    "# â”€â”€â”€ Style Configuration â”€â”€â”€\n",
    "# I'm going to use a consistent color palette across all my plots\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "COLORS = {\n",
    "    'primary':   '#1a365d',   # deep navy\n",
    "    'accent':    '#e53e3e',   # alert red\n",
    "    'positive':  '#276749',   # green\n",
    "    'negative':  '#c53030',   # red\n",
    "    'muted':     '#718096',   # gray\n",
    "    'palette':   ['#1a365d', '#2b6cb0', '#3182ce', '#63b3ed',\n",
    "                  '#e53e3e', '#276749', '#d69e2e', '#805ad5']\n",
    "}\n",
    "\n",
    "# â”€â”€â”€ Stock Universe â”€â”€â”€\n",
    "# I picked 8 stocks across different sectors to get real diversification.\n",
    "# For each one I'm defining mu (expected annual return), sigma (annual volatility),\n",
    "# and a starting price. These params drive the simulation in the next cell.\n",
    "STOCK_UNIVERSE = {\n",
    "    'AAPL':  {'mu': 0.24, 'sigma': 0.28, 'sector': 'Technology',        'price_start': 195},\n",
    "    'MSFT':  {'mu': 0.21, 'sigma': 0.24, 'sector': 'Technology',        'price_start': 420},\n",
    "    'GOOGL': {'mu': 0.18, 'sigma': 0.26, 'sector': 'Technology',        'price_start': 190},\n",
    "    'JPM':   {'mu': 0.15, 'sigma': 0.22, 'sector': 'Financials',        'price_start': 230},\n",
    "    'V':     {'mu': 0.17, 'sigma': 0.19, 'sector': 'Financials',        'price_start': 290},\n",
    "    'JNJ':   {'mu': 0.08, 'sigma': 0.17, 'sector': 'Healthcare',        'price_start': 155},\n",
    "    'XOM':   {'mu': 0.12, 'sigma': 0.25, 'sector': 'Energy',            'price_start': 105},\n",
    "    'PG':    {'mu': 0.10, 'sigma': 0.15, 'sector': 'Consumer Staples',  'price_start': 162},\n",
    "}\n",
    "\n",
    "# SPY benchmark params â€” I'm keeping its volatility lower than most individual stocks,\n",
    "# which makes sense since it's a diversified index\n",
    "BENCHMARK_PARAMS = {'mu': 0.14, 'sigma': 0.15, 'price_start': 590}\n",
    "TICKERS = list(STOCK_UNIVERSE.keys())\n",
    "RISK_FREE_RATE = 0.045  # using the approximate 2025 risk-free rate\n",
    "\n",
    "print(\"âœ“ Configuration loaded\")\n",
    "print(f\"  Stock universe: {len(TICKERS)} equities across \"\n",
    "      f\"{len(set(s['sector'] for s in STOCK_UNIVERSE.values()))} sectors + SPY benchmark\")\n",
    "print(f\"  Risk-free rate: {RISK_FREE_RATE*100}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fad067",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Acquisition & Validation\n",
    "\n",
    "In a real production environment I'd be pulling this from yfinance or a broker API, but for the portfolio I'm simulating the price data using **Cholesky decomposition**. I chose this approach because it lets me bake in realistic correlations between stocks â€” like the fact that AAPL and MSFT should move together way more than AAPL and PG. That matters a lot once I get to the optimization step.\n",
    "\n",
    "After I generate the data, I'm running a quick validation pass to make sure everything looks reasonable before I start analyzing it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c5dbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATA ACQUISITION â€” Simulated with Cholesky Decomposition\n",
    "# ============================================================\n",
    "# In production I'd swap this out for yfinance:\n",
    "#   import yfinance as yf\n",
    "#   df_prices = yf.download(TICKERS + ['SPY'], start='2025-01-01', end='2025-12-31')['Close']\n",
    "\n",
    "# â”€â”€â”€ Step 1: Define the correlation structure â”€â”€â”€\n",
    "# I'm hard-coding a correlation matrix that reflects how these stocks actually\n",
    "# relate to each other. Tech names cluster together, financials pair up,\n",
    "# and the defensive names like JNJ and PG are pretty isolated.\n",
    "CORR_MATRIX = np.array([\n",
    "#        AAPL  MSFT  GOOGL  JPM   V     JNJ   XOM   PG\n",
    "        [1.00, 0.76, 0.73,  0.44, 0.39, 0.14, 0.19, 0.09],  # AAPL\n",
    "        [0.76, 1.00, 0.71,  0.41, 0.37, 0.17, 0.17, 0.11],  # MSFT\n",
    "        [0.73, 0.71, 1.00,  0.39, 0.34, 0.13, 0.15, 0.07],  # GOOGL\n",
    "        [0.44, 0.41, 0.39,  1.00, 0.64, 0.24, 0.29, 0.14],  # JPM\n",
    "        [0.39, 0.37, 0.34,  0.64, 1.00, 0.19, 0.24, 0.17],  # V\n",
    "        [0.14, 0.17, 0.13,  0.24, 0.19, 1.00, 0.09, 0.34],  # JNJ\n",
    "        [0.19, 0.17, 0.15,  0.29, 0.24, 0.09, 1.00, 0.04],  # XOM\n",
    "        [0.09, 0.11, 0.07,  0.14, 0.17, 0.34, 0.04, 1.00],  # PG\n",
    "])\n",
    "\n",
    "# â”€â”€â”€ Step 2: Cholesky decomposition to generate correlated noise â”€â”€â”€\n",
    "# This is the key trick â€” I decompose the correlation matrix into a lower-triangular\n",
    "# matrix L, then I can multiply uncorrelated random draws by L to get correlated ones.\n",
    "# I learned about this in class and it's a really clean way to simulate multi-asset returns.\n",
    "L = np.linalg.cholesky(CORR_MATRIX)\n",
    "\n",
    "trading_days = pd.bdate_range(start='2025-01-02', end='2025-12-31')\n",
    "n_days = len(trading_days)\n",
    "n_stocks = len(TICKERS)\n",
    "\n",
    "# Converting annual params to daily by dividing mu by 252 and sigma by sqrt(252)\n",
    "daily_mu    = np.array([STOCK_UNIVERSE[t]['mu']    for t in TICKERS]) / 252\n",
    "daily_sigma = np.array([STOCK_UNIVERSE[t]['sigma'] for t in TICKERS]) / np.sqrt(252)\n",
    "\n",
    "# I generate uncorrelated standard-normal shocks first, then apply L to correlate them\n",
    "Z_uncorr   = np.random.randn(n_days, n_stocks)\n",
    "Z_corr     = Z_uncorr @ L.T                          # now they're correlated\n",
    "daily_rets = daily_mu + daily_sigma * Z_corr          # daily return = drift + vol * noise\n",
    "\n",
    "# â”€â”€â”€ Step 3: Build price series from the daily returns â”€â”€â”€\n",
    "# I'm using cumulative product here â€” each day's price is last day's price * (1 + return)\n",
    "prices = {}\n",
    "for i, ticker in enumerate(TICKERS):\n",
    "    prices[ticker] = STOCK_UNIVERSE[ticker]['price_start'] * np.cumprod(1 + daily_rets[:, i])\n",
    "\n",
    "# â”€â”€â”€ Step 4: Generate SPY benchmark â”€â”€â”€\n",
    "# I'm tying SPY's movement to the average of all stocks (correlation ~0.7)\n",
    "# with some independent noise mixed in, which felt like a realistic way to simulate it\n",
    "market_avg_ret = daily_rets.mean(axis=1)\n",
    "spy_noise      = np.random.randn(n_days)\n",
    "spy_daily_ret  = (0.70 * market_avg_ret +\n",
    "                  0.30 * (BENCHMARK_PARAMS['mu']/252 + BENCHMARK_PARAMS['sigma']/np.sqrt(252) * spy_noise))\n",
    "prices['SPY'] = BENCHMARK_PARAMS['price_start'] * np.cumprod(1 + spy_daily_ret)\n",
    "\n",
    "# â”€â”€â”€ Assemble everything into one clean DataFrame â”€â”€â”€\n",
    "df_prices = pd.DataFrame(prices, index=trading_days)\n",
    "df_prices.index.name = 'Date'\n",
    "\n",
    "print(\"âœ“ Price data generated\")\n",
    "print(f\"  Trading days: {n_days}  |  Stocks: {n_stocks + 1} (incl. SPY)\")\n",
    "df_prices.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ea1a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATA VALIDATION â€” Making sure the simulated data looks right\n",
    "# ============================================================\n",
    "# I always want to sanity-check my data before I start analyzing it.\n",
    "# I'm going to check for missing values, compare realized returns to my target mus,\n",
    "# and verify the correlation matrix is well-formed.\n",
    "df_returns = df_prices.pct_change().dropna()  # daily returns\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"  DATA VALIDATION REPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check 1: No missing values â€” should be zero since I generated the data myself\n",
    "missing = df_prices.isnull().sum().sum()\n",
    "print(f\"\\n  Missing values:        {missing} {'âœ“' if missing == 0 else 'âœ— ALERT'}\")\n",
    "\n",
    "# Check 2: Comparing what I got vs what I expected for each stock's annual return.\n",
    "# With only 252 trading days of noise, I don't expect these to be perfect matches,\n",
    "# so I'm flagging anything that's more than 15% off from the target mu.\n",
    "realized_annual_ret = ((df_prices.iloc[-1] / df_prices.iloc[0]) - 1)\n",
    "print(f\"\\n  {'Ticker':<8} {'Target Î¼':>10} {'Realized':>10} {'Within 15%?':>12}\")\n",
    "print(f\"  {'â”€'*8} {'â”€'*10} {'â”€'*10} {'â”€'*12}\")\n",
    "for t in TICKERS:\n",
    "    target = STOCK_UNIVERSE[t]['mu']\n",
    "    actual = realized_annual_ret[t]\n",
    "    flag   = 'âœ“' if abs(actual - target) < 0.15 else 'âš '\n",
    "    print(f\"  {t:<8} {target:>9.1%} {actual:>9.1%} {flag:>12}\")\n",
    "\n",
    "# Check 3: The correlation matrix should be symmetric and all values between -1 and 1\n",
    "realized_corr = df_returns[TICKERS].corr()\n",
    "print(f\"\\n  Correlation matrix symmetric: {'âœ“' if np.allclose(realized_corr, realized_corr.T) else 'âœ—'}\")\n",
    "print(f\"  All correlations in [-1, 1]:  {'âœ“' if realized_corr.values.min() >= -1 and realized_corr.values.max() <= 1 else 'âœ—'}\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa967a6",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. SQL Database Layer â€” Storage & Querying\n",
    "\n",
    "I wanted to make sure this project actually uses SQL in a meaningful way, not just as a pass-through. So I'm setting up a normalized SQLite database â€” three tables: prices, metadata, and precomputed daily returns. Then I'm writing queries to pull out the business-level aggregations I need: quarterly breakdowns, sector-level performance, and who the top performer was each quarter.\n",
    "\n",
    "This is the kind of data management workflow I've been practicing in OPAN 6608, and I think it makes the project feel more like something you'd actually do at a company.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19882221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SQL LAYER â€” Creating the database and loading the data in\n",
    "# ============================================================\n",
    "# I'm setting up an in-memory SQLite database with a normalized schema.\n",
    "# Three tables: prices, metadata, and precomputed returns.\n",
    "# In production I'd write this to a file or connect to a real data warehouse.\n",
    "conn = sqlite3.connect(':memory:')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# â”€â”€â”€ Create the tables â”€â”€â”€\n",
    "cursor.executescript(\"\"\"\n",
    "    -- Daily closing prices for every stock\n",
    "    CREATE TABLE stock_prices (\n",
    "        price_date  TEXT NOT NULL,\n",
    "        ticker      TEXT NOT NULL,\n",
    "        close_price REAL NOT NULL,\n",
    "        PRIMARY KEY (price_date, ticker)\n",
    "    );\n",
    "\n",
    "    -- Maps each ticker to its sector â€” keeping this separate is good practice\n",
    "    CREATE TABLE stock_metadata (\n",
    "        ticker  TEXT PRIMARY KEY,\n",
    "        sector  TEXT NOT NULL\n",
    "    );\n",
    "\n",
    "    -- I'm precomputing daily returns so my SQL queries don't have to\n",
    "    CREATE TABLE daily_returns (\n",
    "        return_date TEXT NOT NULL,\n",
    "        ticker      TEXT NOT NULL,\n",
    "        daily_return REAL NOT NULL,\n",
    "        PRIMARY KEY (return_date, ticker)\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "# â”€â”€â”€ Load everything in â”€â”€â”€\n",
    "# Prices â€” one row per stock per day\n",
    "prices_records = [\n",
    "    (str(date.date()), ticker, round(price, 2))\n",
    "    for date in df_prices.index\n",
    "    for ticker, price in df_prices.loc[date].items()\n",
    "]\n",
    "cursor.executemany(\"INSERT INTO stock_prices VALUES (?, ?, ?)\", prices_records)\n",
    "\n",
    "# Metadata â€” just the 8 stocks, not SPY\n",
    "meta_records = [(t, STOCK_UNIVERSE[t]['sector']) for t in TICKERS]\n",
    "cursor.executemany(\"INSERT INTO stock_metadata VALUES (?, ?)\", meta_records)\n",
    "\n",
    "# Returns â€” excluding SPY here since I track the benchmark separately\n",
    "rets_records = [\n",
    "    (str(date.date()), ticker, round(ret, 6))\n",
    "    for date in df_returns.index\n",
    "    for ticker, ret in df_returns.loc[date].items()\n",
    "    if ticker != 'SPY'\n",
    "]\n",
    "cursor.executemany(\"INSERT INTO daily_returns VALUES (?, ?, ?)\", rets_records)\n",
    "\n",
    "conn.commit()\n",
    "print(f\"âœ“ Database populated\")\n",
    "print(f\"  stock_prices:  {cursor.execute('SELECT COUNT(*) FROM stock_prices').fetchone()[0]:,} rows\")\n",
    "print(f\"  daily_returns: {cursor.execute('SELECT COUNT(*) FROM daily_returns').fetchone()[0]:,} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3db261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SQL QUERIES â€” Now let me pull out some useful aggregations\n",
    "# ============================================================\n",
    "\n",
    "# â”€â”€ Query 1: Full-year return and price range per stock â”€â”€\n",
    "# I want to see who had the best annualized return at a glance.\n",
    "# I'm also grabbing the price range (max - min) to get a feel for volatility.\n",
    "query_1 = \"\"\"\n",
    "    SELECT\n",
    "        ticker,\n",
    "        ROUND(AVG(daily_return) * 252, 4)                          AS annualized_avg_return,\n",
    "        ROUND(\n",
    "            (MAX(close_price) - MIN(close_price)) / MIN(close_price),\n",
    "            4\n",
    "        )                                                          AS price_range_pct\n",
    "    FROM daily_returns\n",
    "    JOIN stock_prices USING (ticker)\n",
    "    GROUP BY ticker\n",
    "    ORDER BY annualized_avg_return DESC\n",
    "\"\"\"\n",
    "print(\"â”€â”€ Query 1: Annualized Returns & Price Range â”€â”€\\n\")\n",
    "df_q1 = pd.read_sql(query_1, conn)\n",
    "df_q1.columns = ['Ticker', 'Annualized Return', 'Price Range %']\n",
    "print(df_q1.to_string(index=False))\n",
    "\n",
    "# â”€â”€ Query 2: Sector-level performance â”€â”€\n",
    "# I'm curious which sector did best on average. I'm joining to the metadata table\n",
    "# to get the sector labels, then grouping and averaging.\n",
    "query_2 = \"\"\"\n",
    "    SELECT\n",
    "        m.sector,\n",
    "        COUNT(DISTINCT r.ticker)                                   AS num_stocks,\n",
    "        ROUND(AVG(r.daily_return) * 252, 4)                        AS sector_avg_annual_return,\n",
    "        ROUND(GROUP_CONCAT(DISTINCT r.ticker), 100)                AS tickers\n",
    "    FROM daily_returns r\n",
    "    JOIN stock_metadata m ON r.ticker = m.ticker\n",
    "    GROUP BY m.sector\n",
    "    ORDER BY sector_avg_annual_return DESC\n",
    "\"\"\"\n",
    "print(\"\\nâ”€â”€ Query 2: Sector Performance Summary â”€â”€\\n\")\n",
    "df_q2 = pd.read_sql(query_2, conn)\n",
    "print(df_q2.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a8042e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Query 3: Quarterly performance breakdown â”€â”€\n",
    "# I want to see how each stock did quarter by quarter â€” this is where\n",
    "# I'm using CASE + SUBSTR to assign quarters from the date string.\n",
    "# That's a pattern I picked up from class that's surprisingly handy.\n",
    "query_3 = \"\"\"\n",
    "    WITH quarterly AS (\n",
    "        SELECT\n",
    "            ticker,\n",
    "            CASE\n",
    "                WHEN SUBSTR(return_date, 6, 2) IN ('01','02','03') THEN 'Q1'\n",
    "                WHEN SUBSTR(return_date, 6, 2) IN ('04','05','06') THEN 'Q2'\n",
    "                WHEN SUBSTR(return_date, 6, 2) IN ('07','08','09') THEN 'Q3'\n",
    "                ELSE                                                     'Q4'\n",
    "            END AS quarter,\n",
    "            daily_return\n",
    "        FROM daily_returns\n",
    "    )\n",
    "    SELECT\n",
    "        ticker,\n",
    "        quarter,\n",
    "        ROUND(SUM(daily_return), 4)  AS quarterly_return\n",
    "    FROM quarterly\n",
    "    GROUP BY ticker, quarter\n",
    "    ORDER BY ticker, quarter\n",
    "\"\"\"\n",
    "print(\"â”€â”€ Query 3: Quarterly Return Breakdown â”€â”€\\n\")\n",
    "df_q3 = pd.read_sql(query_3, conn)\n",
    "# Pivoting this into a nice table so I can read it easily\n",
    "df_q3_pivot = df_q3.pivot(index='ticker', columns='quarter', values='quarterly_return')\n",
    "df_q3_pivot = df_q3_pivot[['Q1', 'Q2', 'Q3', 'Q4']]  # enforce the right order\n",
    "df_q3_pivot['Full Year'] = df_q3_pivot.sum(axis=1).round(4)\n",
    "print(df_q3_pivot.to_string())\n",
    "\n",
    "# â”€â”€ Query 4: Top performer each quarter â”€â”€\n",
    "# This one uses a correlated subquery to find the max return per quarter.\n",
    "# I wanted to see if the same stock dominated all year or if it shifted around.\n",
    "query_4 = \"\"\"\n",
    "    WITH quarterly AS (\n",
    "        SELECT\n",
    "            ticker,\n",
    "            CASE\n",
    "                WHEN SUBSTR(return_date, 6, 2) IN ('01','02','03') THEN 'Q1'\n",
    "                WHEN SUBSTR(return_date, 6, 2) IN ('04','05','06') THEN 'Q2'\n",
    "                WHEN SUBSTR(return_date, 6, 2) IN ('07','08','09') THEN 'Q3'\n",
    "                ELSE                                                     'Q4'\n",
    "            END AS quarter,\n",
    "            daily_return\n",
    "        FROM daily_returns\n",
    "    ),\n",
    "    q_totals AS (\n",
    "        SELECT ticker, quarter, ROUND(SUM(daily_return), 4) AS qret\n",
    "        FROM quarterly\n",
    "        GROUP BY ticker, quarter\n",
    "    )\n",
    "    SELECT quarter, ticker AS top_performer, qret AS return\n",
    "    FROM q_totals\n",
    "    WHERE qret = (SELECT MAX(qret) FROM q_totals AS t2 WHERE t2.quarter = q_totals.quarter)\n",
    "    ORDER BY quarter\n",
    "\"\"\"\n",
    "print(\"\\nâ”€â”€ Query 4: Top Performer by Quarter â”€â”€\\n\")\n",
    "df_q4 = pd.read_sql(query_4, conn)\n",
    "print(df_q4.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57fcc9c",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Exploratory Performance Analysis\n",
    "\n",
    "Before I jump into any heavy modeling, I want to just look at how these stocks actually performed over the year. I'm normalizing everything to $100 at the start so I can compare them on the same scale, and plotting them against SPY so I can immediately see who beat the market and by how much.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d339212f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXPLORATORY ANALYSIS â€” Let me visualize cumulative returns\n",
    "# ============================================================\n",
    "# I'm normalizing all prices to $100 at the start of the year.\n",
    "# That way I can plot them all on the same scale and see\n",
    "# who actually did the most relative to where they started.\n",
    "df_normalized = (df_prices / df_prices.iloc[0]) * 100\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "# SPY goes in first as the benchmark reference line â€” thick and dashed\n",
    "ax.plot(df_normalized.index, df_normalized['SPY'],\n",
    "        color=COLORS['muted'], linewidth=2.5, linestyle='--', label='SPY (Benchmark)', zorder=5)\n",
    "\n",
    "# Plotting each stock with its own color\n",
    "for i, ticker in enumerate(TICKERS):\n",
    "    ax.plot(df_normalized.index, df_normalized[ticker],\n",
    "            color=COLORS['palette'][i], linewidth=1.6, alpha=0.85, label=ticker)\n",
    "\n",
    "# I'm adding the final value as a label on the right side of each line\n",
    "# so it's easy to read the ending value without hovering\n",
    "final_vals = df_normalized.iloc[-1].sort_values(ascending=False)\n",
    "for rank, (ticker, val) in enumerate(final_vals.items()):\n",
    "    ax.annotate(f'{val:.0f}',\n",
    "                xy=(df_normalized.index[-1], val),\n",
    "                xytext=(8, 0), textcoords='offset points',\n",
    "                fontsize=8, fontweight='bold',\n",
    "                color=COLORS['primary'] if ticker != 'SPY' else COLORS['muted'],\n",
    "                va='center')\n",
    "\n",
    "ax.set_title('2025 Cumulative Returns â€” Normalized to $100', fontsize=16, fontweight='bold', pad=15)\n",
    "ax.set_ylabel('Value ($)', fontsize=12)\n",
    "ax.set_xlabel('')\n",
    "ax.legend(loc='upper left', fontsize=9, framealpha=0.95)\n",
    "ax.set_xlim(df_normalized.index[0], df_normalized.index[-1])\n",
    "ax.axhline(100, color='gray', linewidth=0.8, linestyle=':')  # baseline\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('01_cumulative_returns.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ“ Saved: 01_cumulative_returns.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a567c3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Performance Summary Table â”€â”€â”€\n",
    "# Let me put together a clean summary table with all the key metrics side by side.\n",
    "# I want return, volatility, and average daily return so I can see the tradeoffs.\n",
    "full_year_return   = (df_prices.iloc[-1] / df_prices.iloc[0] - 1) * 100\n",
    "annualized_vol     = df_returns.std() * np.sqrt(252) * 100\n",
    "avg_daily_return   = df_returns.mean() * 100\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    'Sector':        [STOCK_UNIVERSE.get(t, {}).get('sector', 'Index') for t in df_prices.columns],\n",
    "    'Start Price':   df_prices.iloc[0].round(2),\n",
    "    'End Price':     df_prices.iloc[-1].round(2),\n",
    "    'Return (%)':    full_year_return.round(2),\n",
    "    'Volatility (%)': annualized_vol.round(2),\n",
    "    'Avg Daily Ret (%)': avg_daily_return.round(3)\n",
    "}).sort_values('Return (%)', ascending=False)\n",
    "\n",
    "print(\"â”€â”€ 2025 Performance Summary â”€â”€\\n\")\n",
    "print(summary.to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e9ecd6",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Correlation & Sector Analysis\n",
    "\n",
    "I'm curious how much these stocks actually move together â€” that's going to be a big deal for the portfolio optimization coming up next. I'm going to visualize the correlation matrix as a heatmap, and then I also want to see if I can break down the returns by sector. My hypothesis is that the tech stocks are going to be pretty tightly correlated with each other, and the defensive names like JNJ and PG will be more isolated. Let me see if that holds up.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13d8808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CORRELATION HEATMAP & SECTOR PERFORMANCE\n",
    "# ============================================================\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6.5),\n",
    "                         gridspec_kw={'width_ratios': [1.1, 0.9]})\n",
    "\n",
    "# â”€â”€â”€ Left panel: correlation heatmap â”€â”€â”€\n",
    "# I'm only including the 8 stocks here, not SPY â€” I want to see\n",
    "# how the individual names relate to each other before I optimize.\n",
    "corr = df_returns[TICKERS].corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool), k=1)  # upper triangle for masking\n",
    "\n",
    "sns.heatmap(corr, ax=axes[0], annot=True, fmt='.2f', cmap='RdYlBu_r',\n",
    "            vmin=-0.2, vmax=1.0, linewidths=0.5, linecolor='white',\n",
    "            cbar_kws={'shrink': 0.85, 'label': 'Pearson Correlation'},\n",
    "            annot_kws={'size': 9})\n",
    "axes[0].set_title('Cross-Stock Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "axes[0].tick_params(labelsize=10)\n",
    "\n",
    "# â”€â”€â”€ Right panel: average return by sector â”€â”€â”€\n",
    "# Grouping by sector and averaging â€” I want to see which sector\n",
    "# was the real driver of returns this year.\n",
    "sector_map   = {t: STOCK_UNIVERSE[t]['sector'] for t in TICKERS}\n",
    "sector_rets  = full_year_return[TICKERS].groupby(sector_map).mean().sort_values(ascending=True)\n",
    "\n",
    "bars = axes[1].barh(sector_rets.index, sector_rets.values,\n",
    "                    color=[COLORS['positive'] if v > 0 else COLORS['negative'] for v in sector_rets.values],\n",
    "                    edgecolor='white', linewidth=1.2, height=0.5)\n",
    "\n",
    "# Adding value labels so I don't have to eyeball the axis\n",
    "for bar, val in zip(bars, sector_rets.values):\n",
    "    axes[1].text(val + 0.3, bar.get_y() + bar.get_height()/2,\n",
    "                 f'{val:.1f}%', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "axes[1].axvline(0, color='gray', linewidth=0.8)\n",
    "axes[1].set_title('Average Return by Sector (2025)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Average Return (%)', fontsize=11)\n",
    "axes[1].set_xlim(sector_rets.min() - 3, sector_rets.max() + 4)\n",
    "axes[1].tick_params(labelsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('02_correlation_sector.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ“ Saved: 02_correlation_sector.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840a927b",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Portfolio Optimization â€” Markowitz Mean-Variance\n",
    "\n",
    "This is the part I've been most excited to build. I'm going to use **Markowitz Modern Portfolio Theory** to actually solve for two key portfolios:\n",
    "1. The **Minimum Variance Portfolio** â€” the one that gives me the lowest possible risk\n",
    "2. The **Maximum Sharpe Ratio Portfolio** â€” the one that gives me the best return per unit of risk\n",
    "\n",
    "I also want to trace out the full **Efficient Frontier** â€” the curve of all the optimal portfolios in between. I'm constraining it to long-only (no short selling), with each stock between 1% and 40% weight, so the results feel realistic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9abd62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PORTFOLIO OPTIMIZATION â€” Markowitz Mean-Variance\n",
    "# ============================================================\n",
    "# First I need the annualized return vector and covariance matrix from my 2025 data.\n",
    "# Everything below builds on these two inputs.\n",
    "stock_returns = df_returns[TICKERS]\n",
    "annual_returns = stock_returns.mean() * 252           # expected return vector\n",
    "cov_matrix     = stock_returns.cov() * 252            # annualized covariance matrix\n",
    "n              = len(TICKERS)\n",
    "\n",
    "# â”€â”€â”€ Helper functions for portfolio math â”€â”€â”€\n",
    "# I'm wrapping these up so I can reuse them in the optimizer cleanly.\n",
    "def portfolio_return(weights):\n",
    "    \"\"\"Expected annual return given a weight vector.\"\"\"\n",
    "    return weights @ annual_returns.values\n",
    "\n",
    "def portfolio_volatility(weights):\n",
    "    \"\"\"Annual standard deviation of the portfolio.\"\"\"\n",
    "    return np.sqrt(weights @ cov_matrix.values @ weights)\n",
    "\n",
    "def neg_sharpe(weights):\n",
    "    \"\"\"I negate the Sharpe ratio because scipy only minimizes.\"\"\"\n",
    "    return -(portfolio_return(weights) - RISK_FREE_RATE) / portfolio_volatility(weights)\n",
    "\n",
    "# â”€â”€â”€ Setting up constraints and bounds â”€â”€â”€\n",
    "# Weights have to sum to 1, and I'm capping each stock between 1% and 40%.\n",
    "# The 40% cap prevents the optimizer from just putting everything into one stock.\n",
    "constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}\n",
    "bounds       = tuple((0.01, 0.40) for _ in range(n))\n",
    "initial_w    = np.array([1/n] * n)  # starting guess: equal weight\n",
    "\n",
    "# â”€â”€â”€ Solve 1: Minimum Variance Portfolio â”€â”€â”€\n",
    "# I'm asking scipy to minimize volatility â€” this should push allocation\n",
    "# toward the less correlated, lower-vol names like PG and JNJ.\n",
    "min_var_result = minimize(portfolio_volatility, initial_w,\n",
    "                          method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "min_var_weights = min_var_result.x\n",
    "\n",
    "# â”€â”€â”€ Solve 2: Maximum Sharpe Portfolio â”€â”€â”€\n",
    "# This one maximizes return per unit of risk. I expect it to tilt\n",
    "# toward the higher-returning stocks while still diversifying.\n",
    "max_sharpe_result = minimize(neg_sharpe, initial_w,\n",
    "                             method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "max_sharpe_weights = max_sharpe_result.x\n",
    "\n",
    "print(\"â”€â”€ Optimization Results â”€â”€\\n\")\n",
    "print(f\"  {'Metric':<25} {'Min Variance':>14} {'Max Sharpe':>14}\")\n",
    "print(f\"  {'â”€'*25} {'â”€'*14} {'â”€'*14}\")\n",
    "print(f\"  {'Expected Return':<25} {portfolio_return(min_var_weights):>13.2%} {portfolio_return(max_sharpe_weights):>13.2%}\")\n",
    "print(f\"  {'Volatility':<25} {portfolio_volatility(min_var_weights):>13.2%} {portfolio_volatility(max_sharpe_weights):>13.2%}\")\n",
    "sharpe_mv = (portfolio_return(min_var_weights) - RISK_FREE_RATE) / portfolio_volatility(min_var_weights)\n",
    "sharpe_ms = (portfolio_return(max_sharpe_weights) - RISK_FREE_RATE) / portfolio_volatility(max_sharpe_weights)\n",
    "print(f\"  {'Sharpe Ratio':<25} {sharpe_mv:>14.3f} {sharpe_ms:>14.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8422ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Generating the Efficient Frontier â”€â”€â”€\n",
    "# Now I want to trace out the full curve of optimal portfolios.\n",
    "# I'm sweeping across a range of target returns and for each one,\n",
    "# I ask the optimizer: what's the minimum volatility I can achieve?\n",
    "# The collection of all those points forms the efficient frontier.\n",
    "target_returns = np.linspace(\n",
    "    portfolio_return(min_var_weights),\n",
    "    annual_returns.max() * 0.85,  # I'm capping this below the single best stock\n",
    "    60                             # 60 points gives a smooth curve\n",
    ")\n",
    "\n",
    "frontier_vols, frontier_rets, frontier_weights = [], [], []\n",
    "\n",
    "for target in target_returns:\n",
    "    # Two constraints now: weights sum to 1, AND return hits my target\n",
    "    constraints_ef = [\n",
    "        {'type': 'eq', 'fun': lambda w: np.sum(w) - 1},\n",
    "        {'type': 'eq', 'fun': lambda w, t=target: portfolio_return(w) - t}\n",
    "    ]\n",
    "    res = minimize(portfolio_volatility, initial_w,\n",
    "                   method='SLSQP', bounds=bounds, constraints=constraints_ef)\n",
    "    if res.success:\n",
    "        frontier_vols.append(portfolio_volatility(res.x))\n",
    "        frontier_rets.append(portfolio_return(res.x))\n",
    "        frontier_weights.append(res.x)\n",
    "\n",
    "frontier_vols = np.array(frontier_vols)\n",
    "frontier_rets = np.array(frontier_rets)\n",
    "\n",
    "# I also want to scatter the individual stocks on the same plot\n",
    "# so I can see where each one falls relative to the frontier\n",
    "stock_vols = np.array([cov_matrix.loc[t, t]**0.5 for t in TICKERS])\n",
    "stock_rets = annual_returns.values\n",
    "\n",
    "print(f\"âœ“ Efficient frontier computed: {len(frontier_vols)} portfolios\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff7458c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Plotting the Efficient Frontier and Optimal Portfolios â”€â”€â”€\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6.5))\n",
    "\n",
    "# â”€â”€ Left panel: the frontier itself â”€â”€\n",
    "ax = axes[0]\n",
    "ax.plot(frontier_vols * 100, frontier_rets * 100,\n",
    "        color=COLORS['primary'], linewidth=2.5, label='Efficient Frontier')\n",
    "\n",
    "# Scatter each individual stock so I can see where they land\n",
    "ax.scatter(stock_vols * 100, stock_rets * 100,\n",
    "           c=COLORS['palette'][:n], s=100, edgecolors='white', linewidths=1.5, zorder=5)\n",
    "for i, t in enumerate(TICKERS):\n",
    "    ax.annotate(t, (stock_vols[i]*100, stock_rets[i]*100),\n",
    "                xytext=(6, 4), textcoords='offset points', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Marking the two key portfolios I solved for\n",
    "ax.scatter(portfolio_volatility(min_var_weights)*100, portfolio_return(min_var_weights)*100,\n",
    "           marker='*', s=300, color=COLORS['accent'], edgecolors='white', linewidths=1.5,\n",
    "           zorder=6, label='Min Variance')\n",
    "\n",
    "ax.scatter(portfolio_volatility(max_sharpe_weights)*100, portfolio_return(max_sharpe_weights)*100,\n",
    "           marker='D', s=150, color=COLORS['positive'], edgecolors='white', linewidths=1.5,\n",
    "           zorder=6, label='Max Sharpe')\n",
    "\n",
    "# Drawing the Capital Market Line â€” this is the line from the risk-free rate\n",
    "# through the Max Sharpe portfolio. Everything below it is suboptimal.\n",
    "cml_x = np.linspace(0, max(frontier_vols)*100 * 1.1, 50)\n",
    "cml_slope = (portfolio_return(max_sharpe_weights) - RISK_FREE_RATE) / portfolio_volatility(max_sharpe_weights)\n",
    "cml_y = RISK_FREE_RATE * 100 + cml_slope * cml_x\n",
    "ax.plot(cml_x, cml_y, color=COLORS['muted'], linewidth=1.2, linestyle=':', alpha=0.7, label='Capital Market Line')\n",
    "\n",
    "ax.set_xlabel('Annualized Volatility (%)', fontsize=11)\n",
    "ax.set_ylabel('Annualized Return (%)', fontsize=11)\n",
    "ax.set_title('Efficient Frontier with Optimal Portfolios', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=9, framealpha=0.95)\n",
    "ax.set_xlim(0, max(frontier_vols)*100 * 1.15)\n",
    "\n",
    "# â”€â”€ Right panel: comparing the actual weight allocations â”€â”€\n",
    "# I want to see side by side how Min Variance vs Max Sharpe allocate differently.\n",
    "# I'm also drawing a dashed line at equal weight for reference.\n",
    "ax2 = axes[1]\n",
    "x_pos = np.arange(n)\n",
    "width = 0.35\n",
    "bars1 = ax2.bar(x_pos - width/2, min_var_weights * 100,  width, label='Min Variance',  color=COLORS['accent'],   edgecolor='white')\n",
    "bars2 = ax2.bar(x_pos + width/2, max_sharpe_weights * 100, width, label='Max Sharpe',   color=COLORS['positive'], edgecolor='white')\n",
    "\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels(TICKERS, fontsize=10)\n",
    "ax2.set_ylabel('Weight (%)', fontsize=11)\n",
    "ax2.set_title('Optimal Portfolio Allocations', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.axhline(100/n, color=COLORS['muted'], linewidth=1, linestyle='--', alpha=0.6)\n",
    "ax2.text(n-0.5, 100/n + 0.5, 'Equal Weight', fontsize=8, color=COLORS['muted'], ha='right')\n",
    "\n",
    "# Value labels on each bar\n",
    "for bar in bars1:\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3,\n",
    "             f'{bar.get_height():.1f}%', ha='center', va='bottom', fontsize=7.5, fontweight='bold')\n",
    "for bar in bars2:\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3,\n",
    "             f'{bar.get_height():.1f}%', ha='center', va='bottom', fontsize=7.5, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('03_efficient_frontier.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ“ Saved: 03_efficient_frontier.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5726d7dc",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Risk Metrics & Drawdown Analysis\n",
    "\n",
    "Return numbers alone don't tell the whole story â€” I need to understand the risk side too. I'm going to compute a full set of risk-adjusted metrics for every stock and for SPY:\n",
    "\n",
    "| Metric | What it tells me |\n",
    "|---|---|\n",
    "| **Sharpe Ratio** | How much excess return I'm getting per unit of total risk |\n",
    "| **Sortino Ratio** | Same idea, but it only penalizes me for *downside* risk â€” I like this one better |\n",
    "| **Value at Risk (VaR 95%)** | On a bad day, how much could I lose? (at 95% confidence) |\n",
    "| **Max Drawdown** | The worst peak-to-trough drop over the whole year |\n",
    "| **Calmar Ratio** | Return divided by max drawdown â€” higher means I'm getting paid well for the risk I took |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3bf6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RISK METRICS â€” Computing the full suite\n",
    "# ============================================================\n",
    "# I'm writing helper functions for the trickier metrics so I can reuse them\n",
    "# across stocks and portfolios without repeating code.\n",
    "\n",
    "def compute_sortino(returns, rf_daily):\n",
    "    \"\"\"Sortino only penalizes downside moves â€” I think it's more useful than Sharpe.\"\"\"\n",
    "    excess  = returns - rf_daily\n",
    "    downside = returns[returns < 0]\n",
    "    down_std = downside.std() * np.sqrt(252) if len(downside) > 0 else 1e-6\n",
    "    return (excess.mean() * 252 - RISK_FREE_RATE) / down_std\n",
    "\n",
    "def compute_var_95(returns):\n",
    "    \"\"\"Historical VaR â€” I'm looking at the 5th percentile of daily returns.\"\"\"\n",
    "    return -np.percentile(returns, 5)\n",
    "\n",
    "def compute_max_drawdown(price_series):\n",
    "    \"\"\"Max drawdown: the biggest drop from any peak before recovery.\"\"\"\n",
    "    rolling_max = price_series.cummax()\n",
    "    drawdown    = (price_series - rolling_max) / rolling_max\n",
    "    return drawdown.min()\n",
    "\n",
    "rf_daily = RISK_FREE_RATE / 252\n",
    "\n",
    "# â”€â”€â”€ Loop through every stock + SPY and compute all metrics â”€â”€â”€\n",
    "# I want one clean table at the end I can look at all at once\n",
    "risk_metrics = []\n",
    "for ticker in TICKERS + ['SPY']:\n",
    "    rets = df_returns[ticker]\n",
    "    ann_ret = rets.mean() * 252\n",
    "    ann_vol = rets.std() * np.sqrt(252)\n",
    "    sharpe  = (ann_ret - RISK_FREE_RATE) / ann_vol\n",
    "    sortino = compute_sortino(rets, rf_daily)\n",
    "    var95   = compute_var_95(rets)\n",
    "    mdd     = compute_max_drawdown(df_prices[ticker])\n",
    "    calmar  = ann_ret / abs(mdd) if mdd != 0 else 0\n",
    "\n",
    "    risk_metrics.append({\n",
    "        'Ticker':    ticker,\n",
    "        'Ann Return (%)': round(ann_ret * 100, 2),\n",
    "        'Volatility (%)': round(ann_vol * 100, 2),\n",
    "        'Sharpe':    round(sharpe, 3),\n",
    "        'Sortino':   round(sortino, 3),\n",
    "        'VaR 95% (%)': round(var95 * 100, 3),\n",
    "        'Max DD (%)': round(mdd * 100, 2),\n",
    "        'Calmar':    round(calmar, 3)\n",
    "    })\n",
    "\n",
    "df_risk = pd.DataFrame(risk_metrics).set_index('Ticker')\n",
    "print(\"â”€â”€ Full Risk Metrics Table â”€â”€\\n\")\n",
    "print(df_risk.to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce31cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Risk Dashboard â€” 2x2 grid of key risk visuals â”€â”€â”€\n",
    "# I want to see Sharpe vs Sortino, VaR, max drawdown, and the\n",
    "# risk-return tradeoff all in one place. Let me put this together.\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "stocks_only = df_risk.drop('SPY')\n",
    "spy_row     = df_risk.loc['SPY']\n",
    "colors_bar  = [COLORS['palette'][i] for i in range(len(TICKERS))]\n",
    "\n",
    "# â”€â”€ Top-left: Sharpe vs Sortino side by side â”€â”€\n",
    "# I want to compare these two for each stock â€” Sortino should generally\n",
    "# be higher since it doesn't penalize upside volatility.\n",
    "ax = axes[0, 0]\n",
    "x  = np.arange(len(TICKERS))\n",
    "w  = 0.35\n",
    "ax.bar(x - w/2, stocks_only['Sharpe'],  w, color=COLORS['primary'],  edgecolor='white', label='Sharpe')\n",
    "ax.bar(x + w/2, stocks_only['Sortino'], w, color=COLORS['accent'],   edgecolor='white', label='Sortino')\n",
    "ax.axhline(spy_row['Sharpe'],  color=COLORS['muted'], ls='--', lw=1.2, label=f'SPY Sharpe ({spy_row[\"Sharpe\"]:.2f})')\n",
    "ax.set_xticks(x); ax.set_xticklabels(TICKERS)\n",
    "ax.set_title('Sharpe & Sortino Ratios', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Ratio')\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# â”€â”€ Top-right: VaR at 95% confidence â”€â”€\n",
    "# This tells me the worst daily loss I should expect 5% of the time\n",
    "ax = axes[0, 1]\n",
    "var_vals = stocks_only['VaR 95% (%)']\n",
    "bars = ax.bar(TICKERS, var_vals, color=COLORS['palette'][:len(TICKERS)], edgecolor='white')\n",
    "ax.axhline(spy_row['VaR 95% (%)'], color=COLORS['muted'], ls='--', lw=1.2,\n",
    "           label=f'SPY VaR ({spy_row[\"VaR 95% (%)\"]:.2f}%)')\n",
    "for bar, val in zip(bars, var_vals):\n",
    "    ax.text(bar.get_x()+bar.get_width()/2, bar.get_height()+0.01,\n",
    "            f'{val:.2f}%', ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
    "ax.set_title('Value at Risk (95% Confidence, Daily)', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('VaR (%)')\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# â”€â”€ Bottom-left: Max Drawdown â”€â”€\n",
    "# I'm curious which stocks had the scariest drops during the year\n",
    "ax = axes[1, 0]\n",
    "mdd_vals = stocks_only['Max DD (%)']\n",
    "bars = ax.bar(TICKERS, mdd_vals, color=[COLORS['negative']]*len(TICKERS), edgecolor='white')\n",
    "ax.axhline(spy_row['Max DD (%)'], color=COLORS['muted'], ls='--', lw=1.2,\n",
    "           label=f'SPY Max DD ({spy_row[\"Max DD (%)\"]:.1f}%)')\n",
    "for bar, val in zip(bars, mdd_vals):\n",
    "    ax.text(bar.get_x()+bar.get_width()/2, val - 0.3,\n",
    "            f'{val:.1f}%', ha='center', va='top', fontsize=8, fontweight='bold', color='white')\n",
    "ax.set_title('Maximum Drawdown (2025)', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Drawdown (%)')\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# â”€â”€ Bottom-right: Return vs Risk scatter plot â”€â”€\n",
    "# This is my favorite view â€” it shows the full tradeoff at a glance.\n",
    "# Stocks in the upper-left corner are the ones I actually want to own.\n",
    "ax = axes[1, 1]\n",
    "ax.scatter(stocks_only['Volatility (%)'], stocks_only['Ann Return (%)'],\n",
    "           c=colors_bar, s=120, edgecolors='white', linewidths=1.5, zorder=5)\n",
    "for i, t in enumerate(TICKERS):\n",
    "    ax.annotate(t, (stocks_only.loc[t, 'Volatility (%)'], stocks_only.loc[t, 'Ann Return (%)']),\n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=9, fontweight='bold')\n",
    "# Plotting SPY as a diamond so it stands out as the benchmark\n",
    "ax.scatter(spy_row['Volatility (%)'], spy_row['Ann Return (%)'],\n",
    "           marker='D', s=150, color=COLORS['muted'], edgecolors='white', linewidths=1.5, zorder=5)\n",
    "ax.annotate('SPY', (spy_row['Volatility (%)'], spy_row['Ann Return (%)']),\n",
    "            xytext=(5, 5), textcoords='offset points', fontsize=9, fontweight='bold', color=COLORS['muted'])\n",
    "ax.set_xlabel('Annualized Volatility (%)')\n",
    "ax.set_ylabel('Annualized Return (%)')\n",
    "ax.set_title('Risk-Return Tradeoff', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Risk Metrics Dashboard â€” 2025', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('04_risk_dashboard.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ“ Saved: 04_risk_dashboard.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee9d8ee",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Investment Strategy Comparison\n",
    "\n",
    "Now I want to actually test whether different trading strategies would have performed differently over the year. I'm backtesting five strategies and I'm genuinely curious which one comes out on top:\n",
    "\n",
    "| Strategy | What I'm testing |\n",
    "|---|---|\n",
    "| **Buy & Hold** | The simplest baseline â€” equal weight everything on day one, never touch it |\n",
    "| **Monthly Rebalance** | Same equal weight, but I reset back to equal every month |\n",
    "| **Momentum** | Each month I go long the top-3 stocks by their trailing 30-day return â€” chasing what's been working |\n",
    "| **Risk Parity** | I weight each stock inversely by its recent volatility â€” so the riskier stocks get less allocation |\n",
    "| **Optimized (Max Sharpe)** | I plug in the weights I solved for in the Markowitz section and hold them |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf886fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STRATEGY BACKTESTING â€” Building the engine\n",
    "# ============================================================\n",
    "# I'm writing a generic backtest function that takes any strategy\n",
    "# and replays it day by day over 2025. Each strategy just needs to\n",
    "# return a dict of {ticker: weight} for a given date.\n",
    "\n",
    "def backtest_strategy(strategy_func, name):\n",
    "    \"\"\"\n",
    "    Runs a backtest for any strategy function.\n",
    "    strategy_func(df_returns, df_prices, date) -> {ticker: weight}\n",
    "    Returns cumulative portfolio value as a Series.\n",
    "    \"\"\"\n",
    "    portfolio_value = 1.0\n",
    "    cum_values      = [1.0]\n",
    "    prev_weights    = strategy_func(df_returns, df_prices, df_returns.index[0])\n",
    "\n",
    "    for i in range(1, len(df_returns)):\n",
    "        today_rets = df_returns[TICKERS].iloc[i]\n",
    "        # The portfolio's daily return is just the weighted sum of stock returns\n",
    "        port_ret   = sum(prev_weights.get(t, 0) * today_rets[t] for t in TICKERS)\n",
    "        portfolio_value *= (1 + port_ret)\n",
    "        cum_values.append(portfolio_value)\n",
    "        # Ask the strategy what weights to use tomorrow\n",
    "        prev_weights = strategy_func(df_returns, df_prices, df_returns.index[i])\n",
    "\n",
    "    return pd.Series(cum_values, index=df_returns.index, name=name)\n",
    "\n",
    "# â”€â”€â”€ Strategy 1: Buy & Hold â”€â”€â”€\n",
    "# The simplest possible baseline â€” I just equal-weight everything and never touch it\n",
    "def buy_and_hold(returns, prices, date):\n",
    "    \"\"\"Equal weight, set once, never changes.\"\"\"\n",
    "    return {t: 1/len(TICKERS) for t in TICKERS}\n",
    "\n",
    "# â”€â”€â”€ Strategy 2: Monthly Rebalance â”€â”€â”€\n",
    "# Same as buy and hold but I reset to equal weight every month.\n",
    "# I want to see if that periodic rebalancing actually adds value.\n",
    "def monthly_rebalance(returns, prices, date):\n",
    "    \"\"\"Equal weight, rebalanced monthly.\"\"\"\n",
    "    return {t: 1/len(TICKERS) for t in TICKERS}\n",
    "\n",
    "# â”€â”€â”€ Strategy 3: Momentum â”€â”€â”€\n",
    "# I'm going to try chasing the winners â€” each month I look at which\n",
    "# 3 stocks had the best trailing 30-day return and go all-in on those.\n",
    "# First 30 days I don't have enough history so I default to equal weight.\n",
    "_momentum_cache = {}\n",
    "def momentum_strategy(returns, prices, date):\n",
    "    \"\"\"Top-3 stocks by trailing 30-day return, equal weight among them.\"\"\"\n",
    "    idx = returns.index.get_loc(date)\n",
    "    if idx < 30:\n",
    "        return {t: 1/len(TICKERS) for t in TICKERS}  # warmup period\n",
    "    trail_30 = returns[TICKERS].iloc[idx-30:idx].sum()\n",
    "    top3 = trail_30.nlargest(3).index.tolist()\n",
    "    return {t: (1/3 if t in top3 else 0) for t in TICKERS}\n",
    "\n",
    "# â”€â”€â”€ Strategy 4: Risk Parity â”€â”€â”€\n",
    "# Here I'm flipping the script â€” instead of chasing returns, I'm\n",
    "# weighting inversely by volatility. The idea is that riskier stocks\n",
    "# get less money so each position contributes equally to total risk.\n",
    "def risk_parity(returns, prices, date):\n",
    "    \"\"\"Weight inversely proportional to trailing 60-day volatility.\"\"\"\n",
    "    idx = returns.index.get_loc(date)\n",
    "    if idx < 60:\n",
    "        return {t: 1/len(TICKERS) for t in TICKERS}  # need 60 days of history first\n",
    "    trail_60_vol = returns[TICKERS].iloc[idx-60:idx].std()\n",
    "    inv_vol = 1 / trail_60_vol\n",
    "    total   = inv_vol.sum()\n",
    "    return {t: inv_vol[t] / total for t in TICKERS}\n",
    "\n",
    "print(\"âœ“ Strategy functions defined\")\n",
    "print(\"  Running backtests...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8067342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Running all five strategies â”€â”€â”€\n",
    "# Strategy 1: Buy & Hold â€” the baseline\n",
    "s1_cum = backtest_strategy(buy_and_hold, 'Buy & Hold')\n",
    "\n",
    "# Strategy 2: Monthly Rebalance\n",
    "# I'm handling this one manually month by month because I need to\n",
    "# actually reset the weights at the start of each month to be accurate.\n",
    "monthly_cum = [1.0]\n",
    "monthly_value = 1.0\n",
    "for month in range(1, 13):\n",
    "    mask = df_returns.index.month == month\n",
    "    if mask.sum() == 0: continue\n",
    "    month_rets = df_returns[TICKERS].loc[mask]\n",
    "    w_eq = np.array([1/len(TICKERS)] * len(TICKERS))  # reset to equal weight\n",
    "    port_month_rets = month_rets.values @ w_eq\n",
    "    for dr in port_month_rets:\n",
    "        monthly_value *= (1 + dr)\n",
    "        monthly_cum.append(monthly_value)\n",
    "s2_cum = pd.Series(monthly_cum[:len(df_returns)], index=df_returns.index, name='Monthly Rebalance')\n",
    "\n",
    "# Strategy 3: Momentum â€” chasing the trailing winners\n",
    "s3_cum = backtest_strategy(momentum_strategy, 'Momentum (Top-3)')\n",
    "\n",
    "# Strategy 4: Risk Parity â€” risk-weighted allocation\n",
    "s4_cum = backtest_strategy(risk_parity, 'Risk Parity')\n",
    "\n",
    "# Strategy 5: Max Sharpe â€” using the optimized weights I solved for earlier.\n",
    "# I'm holding those weights for the full year to see how they actually performed.\n",
    "def optimized_maxsharpe(returns, prices, date):\n",
    "    return {t: max_sharpe_weights[i] for i, t in enumerate(TICKERS)}\n",
    "s5_cum = backtest_strategy(optimized_maxsharpe, 'Max Sharpe (Optimized)')\n",
    "\n",
    "# SPY benchmark for comparison\n",
    "spy_rets = df_returns['SPY']\n",
    "spy_cum  = (1 + spy_rets).cumprod()\n",
    "spy_cum  = spy_cum / spy_cum.iloc[0]\n",
    "spy_cum.name = 'SPY Benchmark'\n",
    "\n",
    "# Putting everything into one DataFrame for easy comparison\n",
    "strategies = pd.DataFrame({\n",
    "    'Buy & Hold':          s1_cum,\n",
    "    'Monthly Rebalance':   s2_cum,\n",
    "    'Momentum (Top-3)':    s3_cum,\n",
    "    'Risk Parity':         s4_cum,\n",
    "    'Max Sharpe':          s5_cum,\n",
    "    'SPY Benchmark':       spy_cum\n",
    "})\n",
    "\n",
    "print(\"âœ“ All 5 strategies backtested\")\n",
    "print(f\"\\n  Final values (per $1 invested Jan 1):\\n\")\n",
    "print(strategies.iloc[-1].sort_values(ascending=False).to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024f59d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Computing risk-adjusted metrics for each strategy â”€â”€â”€\n",
    "# I want to compare them on more than just total return â€” I need to see\n",
    "# the Sharpe, drawdown, and Calmar to really understand the tradeoffs.\n",
    "strat_metrics = []\n",
    "for col in strategies.columns:\n",
    "    daily_ret = strategies[col].pct_change().dropna()\n",
    "    ann_ret   = daily_ret.mean() * 252\n",
    "    ann_vol   = daily_ret.std() * np.sqrt(252)\n",
    "    sharpe    = (ann_ret - RISK_FREE_RATE) / ann_vol if ann_vol > 0 else 0\n",
    "    mdd       = compute_max_drawdown(strategies[col])\n",
    "    calmar    = ann_ret / abs(mdd) if mdd != 0 else 0\n",
    "    total_ret = (strategies[col].iloc[-1] - 1) * 100\n",
    "\n",
    "    strat_metrics.append({\n",
    "        'Strategy':      col,\n",
    "        'Total Return (%)': round(total_ret, 2),\n",
    "        'Ann Return (%)':   round(ann_ret * 100, 2),\n",
    "        'Volatility (%)':   round(ann_vol * 100, 2),\n",
    "        'Sharpe Ratio':     round(sharpe, 3),\n",
    "        'Max Drawdown (%)': round(mdd * 100, 2),\n",
    "        'Calmar Ratio':     round(calmar, 3)\n",
    "    })\n",
    "\n",
    "df_strat = pd.DataFrame(strat_metrics).set_index('Strategy').sort_values('Sharpe Ratio', ascending=False)\n",
    "print(\"â”€â”€ Strategy Comparison Metrics â”€â”€\\n\")\n",
    "print(df_strat.to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e376dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Strategy Comparison Dashboard â”€â”€â”€\n",
    "# I want three views: cumulative return lines, total return bars,\n",
    "# and a Sharpe vs Drawdown scatter to see the risk-reward picture.\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "gs  = gridspec.GridSpec(2, 2, hspace=0.3, wspace=0.25)\n",
    "\n",
    "# Color map so each strategy has a consistent color across all plots\n",
    "strat_colors = {\n",
    "    'Buy & Hold':        COLORS['palette'][0],\n",
    "    'Monthly Rebalance': COLORS['palette'][1],\n",
    "    'Momentum (Top-3)':  COLORS['palette'][2],\n",
    "    'Risk Parity':       COLORS['palette'][3],\n",
    "    'Max Sharpe':        COLORS['positive'],\n",
    "    'SPY Benchmark':     COLORS['muted']\n",
    "}\n",
    "\n",
    "# â”€â”€ Top panel: cumulative returns over the full year â”€â”€\n",
    "# I'm making Max Sharpe and SPY thicker so they stand out as the key comparison\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "for col in strategies.columns:\n",
    "    lw = 2.8 if col in ('Max Sharpe', 'SPY Benchmark') else 1.6\n",
    "    ls = '--' if col == 'SPY Benchmark' else '-'\n",
    "    ax1.plot(strategies.index, (strategies[col] - 1) * 100,\n",
    "             color=strat_colors[col], linewidth=lw, linestyle=ls, label=col)\n",
    "\n",
    "ax1.axhline(0, color='gray', linewidth=0.8, linestyle=':')\n",
    "ax1.set_title('Cumulative Returns by Strategy (2025)', fontsize=15, fontweight='bold')\n",
    "ax1.set_ylabel('Return (%)')\n",
    "ax1.legend(loc='upper left', fontsize=9, framealpha=0.95)\n",
    "\n",
    "# â”€â”€ Bottom-left: total return by strategy â”€â”€\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "strat_order  = df_strat.sort_values('Total Return (%)', ascending=True).index\n",
    "x_pos        = np.arange(len(strat_order))\n",
    "bar_colors   = [strat_colors[s] for s in strat_order]\n",
    "\n",
    "bars = ax2.barh(x_pos, df_strat.loc[strat_order, 'Total Return (%)'], color=bar_colors, edgecolor='white', height=0.5)\n",
    "for bar, val in zip(bars, df_strat.loc[strat_order, 'Total Return (%)']):\n",
    "    ax2.text(val + 0.3, bar.get_y() + bar.get_height()/2,\n",
    "             f'{val:.1f}%', va='center', fontsize=9, fontweight='bold')\n",
    "ax2.set_yticks(x_pos)\n",
    "ax2.set_yticklabels(strat_order, fontsize=10)\n",
    "ax2.set_title('Total Return by Strategy', fontsize=13, fontweight='bold')\n",
    "ax2.set_xlabel('Total Return (%)')\n",
    "\n",
    "# â”€â”€ Bottom-right: Sharpe vs Max Drawdown â”€â”€\n",
    "# This is the key \"did it earn enough to justify the pain?\" view\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "for s in df_strat.index:\n",
    "    marker = 'D' if s == 'SPY Benchmark' else 'o'\n",
    "    ax3.scatter(abs(df_strat.loc[s, 'Max Drawdown (%)']),\n",
    "                df_strat.loc[s, 'Sharpe Ratio'],\n",
    "                color=strat_colors[s], s=140, marker=marker,\n",
    "                edgecolors='white', linewidths=1.5, zorder=5)\n",
    "    ax3.annotate(s, (abs(df_strat.loc[s, 'Max Drawdown (%)']), df_strat.loc[s, 'Sharpe Ratio']),\n",
    "                 xytext=(6, 4), textcoords='offset points', fontsize=8.5, fontweight='bold')\n",
    "\n",
    "ax3.set_xlabel('|Max Drawdown| (%)', fontsize=11)\n",
    "ax3.set_ylabel('Sharpe Ratio', fontsize=11)\n",
    "ax3.set_title('Risk-Reward: Sharpe vs Drawdown', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.savefig('05_strategy_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ“ Saved: 05_strategy_comparison.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c6f05b",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Executive Summary\n",
    "\n",
    "I want to wrap everything up in one dashboard that tells the story at a glance â€” best performers, the optimal portfolio allocation, which strategy actually won, and a drawdown timeline so I can see when things got scary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99652d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXECUTIVE SUMMARY â€” Wrapping everything up in one dashboard\n",
    "# ============================================================\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# â”€â”€ Top-left: Top 5 stocks by return â”€â”€\n",
    "# I want to see at a glance who the winners were, with SPY as a reference line\n",
    "ax = axes[0, 0]\n",
    "top5 = full_year_return[TICKERS].nlargest(5).sort_values(ascending=True)\n",
    "bars = ax.barh(top5.index, top5.values,\n",
    "               color=[COLORS['positive'] if v > 0 else COLORS['negative'] for v in top5.values],\n",
    "               edgecolor='white', height=0.5)\n",
    "ax.axvline(full_year_return['SPY'], color=COLORS['muted'], ls='--', lw=1.5,\n",
    "           label=f'SPY ({full_year_return[\"SPY\"]:.1f}%)')\n",
    "for bar, val in zip(bars, top5.values):\n",
    "    ax.text(val + 0.2, bar.get_y() + bar.get_height()/2,\n",
    "            f'{val:.1f}%', va='center', fontsize=9, fontweight='bold')\n",
    "ax.set_title('Top 5 Stocks by 2025 Return', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "# â”€â”€ Top-right: Max Sharpe portfolio allocation as a pie chart â”€â”€\n",
    "# I'm curious to see the final allocation visually â€” which stocks\n",
    "# did the optimizer actually want to own?\n",
    "ax = axes[0, 1]\n",
    "wedge_colors = COLORS['palette'][:len(TICKERS)]\n",
    "explode = [0.04] * len(TICKERS)\n",
    "wedges, texts, autotexts = ax.pie(\n",
    "    max_sharpe_weights, labels=TICKERS, autopct='%1.1f%%',\n",
    "    colors=wedge_colors, explode=explode, startangle=90,\n",
    "    textprops={'fontsize': 9}\n",
    ")\n",
    "for at in autotexts:\n",
    "    at.set_fontsize(8)\n",
    "    at.set_fontweight('bold')\n",
    "ax.set_title('Max Sharpe Portfolio â€” Allocation', fontsize=13, fontweight='bold')\n",
    "\n",
    "# â”€â”€ Bottom-left: Key findings text box â”€â”€\n",
    "# Pulling the winners from each category so I can summarize the takeaways\n",
    "ax = axes[1, 0]\n",
    "ax.axis('off')\n",
    "best_return  = df_strat['Total Return (%)'].idxmax()\n",
    "best_sharpe  = df_strat['Sharpe Ratio'].idxmax()\n",
    "best_calmar  = df_strat['Calmar Ratio'].idxmax()\n",
    "least_mdd    = df_strat['Max Drawdown (%)'].idxmax()\n",
    "\n",
    "summary_text = (\n",
    "    f\"KEY FINDINGS\\n\"\n",
    "    f\"{'â”€'*38}\\n\\n\"\n",
    "    f\"ðŸ†  Best Total Return:     {best_return}\\n\"\n",
    "    f\"        ({df_strat.loc[best_return, 'Total Return (%)']:.1f}%)\\n\\n\"\n",
    "    f\"ðŸ“Š  Best Risk-Adjusted:    {best_sharpe}\\n\"\n",
    "    f\"        (Sharpe: {df_strat.loc[best_sharpe, 'Sharpe Ratio']:.3f})\\n\\n\"\n",
    "    f\"ðŸ›¡ï¸   Safest (Calmar):      {best_calmar}\\n\"\n",
    "    f\"        (Calmar: {df_strat.loc[best_calmar, 'Calmar Ratio']:.3f})\\n\\n\"\n",
    "    f\"ðŸ“ˆ  Max Sharpe Portfolio\\n\"\n",
    "    f\"        Return: {portfolio_return(max_sharpe_weights)*100:.1f}%  |  \"\n",
    "    f\"Vol: {portfolio_volatility(max_sharpe_weights)*100:.1f}%  |  \"\n",
    "    f\"Sharpe: {sharpe_ms:.3f}\"\n",
    ")\n",
    "ax.text(0.05, 0.95, summary_text, transform=ax.transAxes,\n",
    "        fontsize=11, verticalalignment='top', fontfamily='monospace',\n",
    "        bbox=dict(boxstyle='round,pad=0.5', facecolor=COLORS['palette'][0], alpha=0.08, edgecolor=COLORS['primary']))\n",
    "\n",
    "# â”€â”€ Bottom-right: Drawdown timeline for the best risk-adjusted strategy â”€â”€\n",
    "# I want to see when that strategy actually got hit hardest during the year\n",
    "ax = axes[1, 1]\n",
    "best_strat_col = df_strat['Sharpe Ratio'].idxmax()\n",
    "best_series    = strategies[best_strat_col]\n",
    "rolling_max    = best_series.cummax()\n",
    "drawdown       = (best_series - rolling_max) / rolling_max * 100\n",
    "\n",
    "ax.fill_between(drawdown.index, drawdown.values, 0, color=COLORS['negative'], alpha=0.35)\n",
    "ax.plot(drawdown.index, drawdown.values, color=COLORS['negative'], linewidth=1.2)\n",
    "ax.set_title(f'Drawdown Timeline â€” {best_strat_col}', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Drawdown (%)')\n",
    "\n",
    "plt.suptitle('2025 Investment Analysis â€” Executive Summary',\n",
    "             fontsize=17, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('06_executive_summary.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ“ Saved: 06_executive_summary.png\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"  ANALYSIS COMPLETE\")\n",
    "print(\"=\"*60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
